{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3yeJGnCYxuF"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## Modelo de lenguaje con tokenización por caracteres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv5PEwGzZA9-"
      },
      "source": [
        "### Consigna\n",
        "- Seleccionar un corpus de texto sobre el cual entrenar el modelo de lenguaje.\n",
        "- Realizar el pre-procesamiento adecuado para tokenizar el corpus, estructurar el dataset y separar entre datos de entrenamiento y validación.\n",
        "- Proponer arquitecturas de redes neuronales basadas en unidades recurrentes para implementar un modelo de lenguaje.\n",
        "- Con el o los modelos que consideren adecuados, generar nuevas secuencias a partir de secuencias de contexto con las estrategias de greedy search y beam search determístico y estocástico. En este último caso observar el efecto de la temperatura en la generación de secuencias.\n",
        "\n",
        "\n",
        "### Sugerencias\n",
        "- Durante el entrenamiento, guiarse por el descenso de la perplejidad en los datos de validación para finalizar el entrenamiento. Para ello se provee un callback.\n",
        "- Explorar utilizar SimpleRNN (celda de Elman), LSTM y GRU.\n",
        "- rmsprop es el optimizador recomendado para la buena convergencia. No obstante se pueden explorar otros.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y-QdFbHZYj7C"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-17 18:19:15.372494: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-04-17 18:19:15.615875: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-04-17 18:19:15.820381: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744924756.012656   63536 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744924756.069424   63536 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1744924756.458787   63536 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1744924756.458822   63536 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1744924756.458828   63536 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1744924756.458833   63536 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import io\n",
        "import pickle\n",
        "import lxml\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTvXlEKQZdqx"
      },
      "source": [
        "### Datos\n",
        "Utilizaremos como dataset canciones de bandas de habla inglés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7amy6uUaBLVD"
      },
      "outputs": [],
      "source": [
        "# descargar de textos.info\n",
        "import urllib.request\n",
        "\n",
        "# Para leer y parsear el texto en HTML de wikipedia\n",
        "import bs4 as bs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6v_ickFwBJTy"
      },
      "outputs": [],
      "source": [
        "raw_html = urllib.request.urlopen('https://www.textos.info/julio-verne/la-vuelta-al-mundo-en-80-dias/ebook')\n",
        "raw_html = raw_html.read()\n",
        "\n",
        "# Parsear artículo, 'lxml' es el parser a utilizar\n",
        "article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
        "\n",
        "# Encontrar todos los párrafos del HTML (bajo el tag <p>)\n",
        "# y tenerlos disponible como lista\n",
        "article_paragraphs = article_html.find_all('p')\n",
        "\n",
        "article_text = ''\n",
        "\n",
        "for para in article_paragraphs:\n",
        "    article_text += para.text + ' '\n",
        "\n",
        "# pasar todo el texto a minúscula\n",
        "article_text = article_text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WBE0sSYuB-E6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' en el año 1872, la casa número 7 de saville-row, burlington gardens \\r\\n—donde murió sheridan en 1814— estaba habitada por phileas fogg, quien a\\r\\n pesar de que parecía haber tomado el partido de no hacer nada que \\r\\npudiese llamar la atención, era uno de los miembros más notables y \\r\\nsingulares del reformclub de londres. por consiguiente, phileas fogg, personaje enigmático y del cual sólo \\r\\nse sabía que era un hombre muy galante y de los más cumplidos gentlemen \\r\\nde la alta sociedad inglesa, sucedía a uno de los más grandes oradores \\r\\nque honran a inglaterra. decíase que se daba un aire a lo byron —su cabeza, se entiende, \\r\\nporque, en cuanto a los pies, no tenía defecto alguno—, pero a un byron \\r\\nde bigote y pastillas, a un byron impasible, que hubiera vivido mil años\\r\\n sin envejecer. phileas fogg, era inglés de pura cepa; pero quizás no había nacido en\\r\\n londres. jamás se le había visto en la bolsa ni en el banco, ni en \\r\\nninguno de los despachos mercantiles de la city. ni las dársenas '"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# en article text se encuentra el texto de todo el libro\n",
        "article_text[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "399771"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(article_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-17 18:22:20--  http://gull_corpus.csv/\n",
            "Resolving gull_corpus.csv (gull_corpus.csv)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘gull_corpus.csv’\n",
            "--2025-04-17 18:22:20--  https://raw.githubusercontent.com/karen-pal/borges/refs/heads/master/datasets/full_corpus.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6022847 (5,7M) [text/plain]\n",
            "Saving to: ‘full_corpus.csv’\n",
            "\n",
            "full_corpus.csv     100%[===================>]   5,74M  --.-KB/s    in 0,06s   \n",
            "\n",
            "2025-04-17 18:22:22 (102 MB/s) - ‘full_corpus.csv’ saved [6022847/6022847]\n",
            "\n",
            "FINISHED --2025-04-17 18:22:22--\n",
            "Total wall clock time: 1,2s\n",
            "Downloaded: 1 files, 5,7M in 0,06s (102 MB/s)\n"
          ]
        }
      ],
      "source": [
        "# Descargar la carpeta de dataset\n",
        "import os\n",
        "import platform\n",
        "if os.access('full_corpus.csv', os.F_OK) is False:\n",
        "        if platform.system() == 'Windows':\n",
        "            !curl https://raw.githubusercontent.com/karen-pal/borges/refs/heads/master/datasets/full_corpus.csv -o full_corpus.csv\n",
        "        else:\n",
        "            !wget gull_corpus.csv https://raw.githubusercontent.com/karen-pal/borges/refs/heads/master/datasets/full_corpus.csv\n",
        "\n",
        "else:\n",
        "    print(\"El dataset ya se encuentra descargado\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'¿Qué soñará el indescifrable futuro? Soñará que Alonso Quijano puede ser don Quijote sin dejar su aldea y sus libros. Soñará que una víspera de Ulises puede ser más pródiga que el poema que narra sus trabajos. Soñará generaciones humanas que no reconocerán el nombre de Ulises. Soñará sueños más precisos que la vigilia de hoy. Soñará que podremos hacer milagros y que no los haremos, porque será más real imaginarlos. Soñará mundos tan intensos que la voz de una sola de sus aves podría matarte. Soñará que el olvido y la memoria pueden ser actos voluntarios, no agresiones o dádivas del azar. Soñará que veremos con todo el cuerpo, como quería Milton desde la sombra de esos tiernos orbes, los ojos. Soñará un mundo sin la máquina y sin esa doliente máquina, el cuerpo. La vida no es un sueño pero puede llegar a ser un sueño, escribe Novalis. FIN'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('full_corpus.csv').drop(columns=[\"Unnamed: 0\"])\n",
        "df_borges = df[df.author == 'Jorge Luis Borges']\n",
        "df_borges\n",
        "df_borges_1 = df_borges.iloc[2]\n",
        "df_borges_1.text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "é también de olvidar. Abel dijo despacio: —Así es. Mientras dura el remordimiento dura la culpa. FIN\n",
            "nos costaba un poco de trabajo, pero luego nos perfeccionamos tanto que nos perdíamos enseguida. FIN\n",
            "te máquina, el cuerpo. La vida no es un sueño pero puede llegar a ser un sueño, escribe Novalis. FIN\n",
            "incapaz de jinetear un bagual, pero le gustan los caballos y los entiende. Es amigo de un indio. FIN\n",
            "nueve, ocho, siete, seis, cinco, etcétera. Ese número entero es inconcebible; ergo, Dios existe. FIN\n",
            " valiente y se puso a pelear contra los soldados junto al desertor Martín Fierro. FIN El aleph, 1949\n",
            " valiente y se puso a pelear contra los soldados junto al desertor Martín Fierro. FIN El aleph, 1949\n",
            " Geográficas. Suárez Miranda, Viajes de Varones Prudentes, Libro Cuarto, Cap. XLV, Lérida, 1658. FIN\n",
            " no se resolvieron A (ya en plena mística)- Francamente no recuerdo si esa noche nos suicidamos. FIN\n",
            "ue Almotásim es el «hindú» que el estudiante cree haber matado.   FIN Historia de la eternidad, 1936\n",
            " examinador le pregunta si será reprobado o si pasará. El candidato responde que será reprobado… FIN\n",
            "roja de las plumas; mi turbante, con el más califa de los rubíes.” A Estela Canto FIN El Aleph, 1949\n",
            "l éxtasis o si alcanzó a reconocer, siquiera como una criatura o un perro, los padres y la casa. FIN\n",
            " estaba sin techo; habían arrancado las vigas para construir la Cruz. FIN El informe de Brodie, 1970\n",
            " dicho era el otro: no tenía destino sobre la tierra y había matado a un hombre. FIN Ficciones, 1944\n",
            " 1. Doy a la ch el valor que tiene la palabra loch. (Nota del Autor). FIN El informe de Brodie, 1970\n",
            "efendía que Homero es un personaje simbólico, a la manera de Plutón o de Aquiles. FIN El Aleph, 1949\n",
            "ia, le causó heridas que determinaron su muerte. FIN El jardín de los senderos que se bifurcan, 1941\n",
            ". Siento un poco de alivio, pero no quiero ni pasar por la calle México. FIN El libro de arena, 1975\n",
            "ir Hladík murió el veintinueve de marzo, a las nueve y dos minutos de la mañana. FIN Ficciones, 1944\n",
            "erto, porque para Bandeira ya estaba muerto. Suárez, casi con desdén, hace fuego. FIN El Aleph, 1944\n",
            "s cuando nada nos toca, ni una palabra, ni un anhelo, ni una memoria. Yo sé que no estoy muerto. FIN\n",
            "y otro a la izquierda. Suele tener seis patas porque las delanteras sirven para los dos cuerpos. FIN\n",
            "a lástima. Tanta dureza, tanta fe, tan apacible o inocente soberbia, y los años pasan, inútiles. FIN\n",
            "empuña con firmeza el cuchillo, que acaso no sabrá manejar, y sale a la llanura. FIN Ficciones, 1944\n",
            "decido; solo eran falsas las circunstancias, la hora y uno o dos nombres propios. FIN El Aleph, 1949\n",
            "cer nada. -Puedo hacer una cosa -le contesté. -¿Cuál? -me preguntó. -Despertarme. Y así lo hice. FIN\n",
            "l Infierno, versos 97-102, donde se comparan la visión profética y la presbicia. FIN Ficciones, 1944\n",
            "licar ademanes inútiles. Ireneo Funes murió en 1889, de una congestión pulmonar. FIN Ficciones, 1944\n",
            "a me he afincado aquí. San Telmo ha sido siempre un barrio de orden.” FIN El informe de Brodie, 1970\n",
            "e von Kühlmann 1. Tambíén Gibbon (Decline and Fall, XLV) transcribe estos versos. FIN El Aleph, 1949\n",
            "o nuevo, inocente, y no quedaba ni un rastrito de sangre. FIN Historia universal de la infamia, 1935\n",
            "ballo o de león. Su lugar está en el cortejo de las divinidades marinas, junto a los hipocampos. FIN\n",
            " ahora la memoria de un cuchillo y mañana el olvido, el común olvido. FIN El informe de Brodie, 1970\n",
            " desdoblaría en otras análogas; la inconcebible hoja central no tendría revés.   FIN Ficciones, 1944\n",
            "fago” haya donado el oro y haya retirado la bolsa. 2- Bastaría, en rigor, con un solo mono inmortal.\n",
            "os para inferir que en boca de Asterión, ese adjetivo numeral vale por infinitos. FIN El Aleph, 1944\n",
            " eso dejo que me olviden los días, acostado en la oscuridad. FIN A Emma Risso Platero El Aleph, 1949\n",
            "o he denunciado al hombre que me amparó: yo soy Vincent Moon. Ahora desprécieme. FIN Ficciones, 1944\n",
            "rculo: la mujer tristemente sacrificada y la obligación de olvidarla. FIN El informe de Brodie, 1970\n",
            "a. Nos elude de segundo en segundo. La sentencia del romano se gasta, las noches roen el mármol. FIN\n",
            "a corporación, porque Babilonia no es otra cosa que un infinito juego de azares. FIN Ficciones, 1944\n",
            "ible, incesante. Retrocedió unos pasos. Después, muy cuidadosamente, hizo fuego. FIN Ficciones, 1944\n",
            "aba su corazón, y tardó mucho en conseguirlo, y acaso no hay mayores felicidades. FIN El Aleph, 1949\n",
            "n y festejaron su muerte con tambores, con pífanos y danzas. FIN Manual de zoología fantástica, 1957\n",
            " que el odio de los ángeles…” El fin del manuscrito no se ha encontrado. FIN El libro de arena, 1975\n",
            "e oírlas, no leerlas): ¡Pero, che! Lo matan y no sabe que muere para que se repita una escena. FIN  \n",
            "terror, comprendió que él también era una apariencia, que otro estaba soñándolo. FIN Ficciones, 1944\n",
            "o, que tantos hombres he sido, no he sido nunca aquel en cuyo abrazo desfallecía Matilde Urbach. FIN\n",
            "l desierto, donde murió de hambre y de sed. La gloria sea con aquel que no muere. FIN El Aleph, 1944\n",
            "de vista a su muerte. A veces unos pájaros, un caballo, han salvado las ruinas de un anfiteatro. FIN\n",
            "ersona. 1. En las cruces rúnicas los dos emblemas enemigos conviven entrelazados. FIN El Aleph, 1949\n",
            "o borre o que la memoria no altere y cuando nadie sabe en qué imágenes lo traducirá el porvenir. FIN\n",
            "A unos pasos de la casa del rey, Odín había muerto. FIN Delia Ingenieros es coautora de este cuento.\n",
            "as de Ariosto. Porque en el principio de la literatura está el mito, y así mismo en el fin. FIN 1955\n",
            "rrabales de Nîmes; solía llevar consigo un cuaderno y hacer una alegre fogata.   FIN Ficciones, 1944\n",
            "n libro dedicado a la gloria del héroe; también eso, tal vez, estaba previsto. FIN Ficciones, 1944  \n",
            "hakespeare. 5. Buckley era librepensador, fatalista y defensor de la esclavitud. FIN Ficciones, 1944\n",
            " capítulo del primer tomo de la Vindicación de la eternidad, de Jaromir Hladík). FIN Ficciones, 1944\n",
            "e le pesa en la mano y él mismo y toda su vida pretérita y los vastos dioses y el universo. FIN 1960\n",
            "otra celda circular… El proceso no tiene fin y nadie podrá leer lo que los prisioneros escriben. FIN\n",
            "rá, dentro de miles de años, con materiales hoy dispersos en el planeta. FIN El libro de arena, 1975\n"
          ]
        }
      ],
      "source": [
        "for fin in df_borges['text'].str[-100:]:\n",
        "    print(fin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>link</th>\n",
              "      <th>text</th>\n",
              "      <th>title</th>\n",
              "      <th>metadata</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>https://ciudadseva.com/texto/abel-y-cain-borges/</td>\n",
              "      <td>Abel y Caín se encontraron después de la muert...</td>\n",
              "      <td>Abel y Caín</td>\n",
              "      <td>[Minicuento - Texto completo.]</td>\n",
              "      <td>Jorge Luis Borges</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>https://ciudadseva.com/texto/adrogue/</td>\n",
              "      <td>Era muy lindo, un pueblo laberíntico. A veces,...</td>\n",
              "      <td>Adrogué</td>\n",
              "      <td>[Minicuento - Texto completo.]</td>\n",
              "      <td>Jorge Luis Borges</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>https://ciudadseva.com/texto/alguien-sonara/</td>\n",
              "      <td>¿Qué soñará el indescifrable futuro? Soñará qu...</td>\n",
              "      <td>Alguien soñará</td>\n",
              "      <td>[Minicuento - Texto completo.]</td>\n",
              "      <td>Jorge Luis Borges</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>https://ciudadseva.com/texto/andres-armoa/</td>\n",
              "      <td>Los años le han dejado unas palabras en guaran...</td>\n",
              "      <td>Andrés Armoa</td>\n",
              "      <td>[Minicuento - Texto completo.]</td>\n",
              "      <td>Jorge Luis Borges</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>https://ciudadseva.com/texto/argumentum-ornith...</td>\n",
              "      <td>Cierro los ojos y veo una bandada de pájaros. ...</td>\n",
              "      <td>Argumentum ornithologicum</td>\n",
              "      <td>[Minicuento - Texto completo.]</td>\n",
              "      <td>Jorge Luis Borges</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>https://ciudadseva.com/texto/tlon-uqbar-orbis-...</td>\n",
              "      <td>I Debo a la conjunción de un espejo y de una e...</td>\n",
              "      <td>Tlön, Uqbar, Orbis Tertius</td>\n",
              "      <td>[Cuento - Texto completo.]</td>\n",
              "      <td>Jorge Luis Borges</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>https://ciudadseva.com/texto/tres-versiones-de...</td>\n",
              "      <td>En el Asia Menor o en Alejandría, en el segund...</td>\n",
              "      <td>Tres versiones de Judas</td>\n",
              "      <td>[Cuento - Texto completo.]</td>\n",
              "      <td>Jorge Luis Borges</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>https://ciudadseva.com/texto/un-problema/</td>\n",
              "      <td>Imaginemos que en Toledo se descubre un papel ...</td>\n",
              "      <td>Un problema</td>\n",
              "      <td>[Minicuento - Texto completo.]</td>\n",
              "      <td>Jorge Luis Borges</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>https://ciudadseva.com/texto/un-sueno-2/</td>\n",
              "      <td>En un desierto lugar del Irán hay una no muy a...</td>\n",
              "      <td>Un sueño</td>\n",
              "      <td>[Minicuento - Texto completo.]</td>\n",
              "      <td>Jorge Luis Borges</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>https://ciudadseva.com/texto/utopia-de-un-homb...</td>\n",
              "      <td>No hay dos cerros iguales, pero en cualquier l...</td>\n",
              "      <td>Utopía de un hombre que está cansado</td>\n",
              "      <td>[Cuento - Texto completo.]</td>\n",
              "      <td>Jorge Luis Borges</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>62 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  link  \\\n",
              "144   https://ciudadseva.com/texto/abel-y-cain-borges/   \n",
              "145              https://ciudadseva.com/texto/adrogue/   \n",
              "146       https://ciudadseva.com/texto/alguien-sonara/   \n",
              "147         https://ciudadseva.com/texto/andres-armoa/   \n",
              "148  https://ciudadseva.com/texto/argumentum-ornith...   \n",
              "..                                                 ...   \n",
              "201  https://ciudadseva.com/texto/tlon-uqbar-orbis-...   \n",
              "202  https://ciudadseva.com/texto/tres-versiones-de...   \n",
              "203          https://ciudadseva.com/texto/un-problema/   \n",
              "204           https://ciudadseva.com/texto/un-sueno-2/   \n",
              "205  https://ciudadseva.com/texto/utopia-de-un-homb...   \n",
              "\n",
              "                                                  text  \\\n",
              "144  Abel y Caín se encontraron después de la muert...   \n",
              "145  Era muy lindo, un pueblo laberíntico. A veces,...   \n",
              "146  ¿Qué soñará el indescifrable futuro? Soñará qu...   \n",
              "147  Los años le han dejado unas palabras en guaran...   \n",
              "148  Cierro los ojos y veo una bandada de pájaros. ...   \n",
              "..                                                 ...   \n",
              "201  I Debo a la conjunción de un espejo y de una e...   \n",
              "202  En el Asia Menor o en Alejandría, en el segund...   \n",
              "203  Imaginemos que en Toledo se descubre un papel ...   \n",
              "204  En un desierto lugar del Irán hay una no muy a...   \n",
              "205  No hay dos cerros iguales, pero en cualquier l...   \n",
              "\n",
              "                                    title                        metadata  \\\n",
              "144                           Abel y Caín  [Minicuento - Texto completo.]   \n",
              "145                               Adrogué  [Minicuento - Texto completo.]   \n",
              "146                        Alguien soñará  [Minicuento - Texto completo.]   \n",
              "147                          Andrés Armoa  [Minicuento - Texto completo.]   \n",
              "148             Argumentum ornithologicum  [Minicuento - Texto completo.]   \n",
              "..                                    ...                             ...   \n",
              "201            Tlön, Uqbar, Orbis Tertius      [Cuento - Texto completo.]   \n",
              "202               Tres versiones de Judas      [Cuento - Texto completo.]   \n",
              "203                           Un problema  [Minicuento - Texto completo.]   \n",
              "204                              Un sueño  [Minicuento - Texto completo.]   \n",
              "205  Utopía de un hombre que está cansado      [Cuento - Texto completo.]   \n",
              "\n",
              "                author  \n",
              "144  Jorge Luis Borges  \n",
              "145  Jorge Luis Borges  \n",
              "146  Jorge Luis Borges  \n",
              "147  Jorge Luis Borges  \n",
              "148  Jorge Luis Borges  \n",
              "..                 ...  \n",
              "201  Jorge Luis Borges  \n",
              "202  Jorge Luis Borges  \n",
              "203  Jorge Luis Borges  \n",
              "204  Jorge Luis Borges  \n",
              "205  Jorge Luis Borges  \n",
              "\n",
              "[62 rows x 5 columns]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_borges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "texto_concatenado = ' '.join(df_borges['text'].astype(str))\n",
        "texto_concatenado = texto_concatenado.replace('FIN', '')\n",
        "\n",
        "with open('texto_concatenado.csv', 'w', encoding='utf-8') as f:\n",
        "    f.write(texto_concatenado)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "author\n",
              "Jorge Luis Borges            62\n",
              "Julio Cortázar               55\n",
              "Baldomero Lillo              50\n",
              "Augusto Monterroso           45\n",
              "Juan José Arreola            45\n",
              "Alfonso Reyes                37\n",
              "Enrique Anderson Imbert      36\n",
              "Mario Benedetti              33\n",
              "Julio Ramón Ribeyro          27\n",
              "Clarice Lispector            25\n",
              "Roberto Arlt                 25\n",
              "Julio Torri                  24\n",
              "Felisberto Hernández         15\n",
              "Luis Vidales                 14\n",
              "Adolfo Bioy Casares          13\n",
              "Rubén Darío                  13\n",
              "Álvaro Mutis                 11\n",
              "Edmundo Valadés              10\n",
              "Juan Rodolfo Wilcock         10\n",
              "Juan Rulfo                   10\n",
              "Salarrué                      9\n",
              "Elena Garro                   9\n",
              "Manuel A. Alonso              9\n",
              "Alejo Carpentier              9\n",
              "Juan Bosch                    8\n",
              "Eduardo Gudiño Kieffer        8\n",
              "Virgilio Díaz Grullón         7\n",
              "Silvina Ocampo                7\n",
              "Andrés Rivera                 7\n",
              "Rodolfo Walsh                 6\n",
              "Ricardo Güiraldes             6\n",
              "Pablo Palacio                 5\n",
              "Manuel Romero de Terreros     5\n",
              "José Donoso                   5\n",
              "Julio Garmendia               4\n",
              "Gregorio López y Fuentes      4\n",
              "Octavio Paz                   3\n",
              "José María Arguedas           3\n",
              "Sergio Pitol                  3\n",
              "Amparo Dávila                 3\n",
              "Vicente Huidobro              3\n",
              "María Luisa Bombal            3\n",
              "José Lezama Lima              3\n",
              "Rómulo Gallegos               3\n",
              "Macedonio Fernández           3\n",
              "Inés Arredondo                3\n",
              "Virgilio Piñera               3\n",
              "José Edwards                  3\n",
              "Rubén Bareiro Saguier         3\n",
              "Leonora Carrington            2\n",
              "Santiago Dabove               2\n",
              "Humberto Arenal               2\n",
              "Teresa de la Parra            2\n",
              "Ricardo Jaimes Freyre         1\n",
              "Carmen Lyra                   1\n",
              "Esteban Echeverría            1\n",
              "Manuel González Zeledón       1\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.author.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "506942"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(texto_concatenado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "article_text = texto_concatenado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP1JdiOIKQWi"
      },
      "source": [
        "### Elegir el tamaño del contexto\n",
        "\n",
        "En este caso, como el modelo de lenguaje es por caracteres, todo un gran corpus\n",
        "de texto puede ser considerado un documento en sí mismo y el tamaño de contexto\n",
        "puede ser elegido con más libertad en comparación a un modelo de lenguaje tokenizado por palabras y dividido en documentos más acotados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wumBNwdjJM3j"
      },
      "outputs": [],
      "source": [
        "# seleccionamos el tamaño de contexto\n",
        "max_context_size = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "m5FeTaGvbDbw"
      },
      "outputs": [],
      "source": [
        "# Usaremos las utilidades de procesamiento de textos y secuencias de Keras\n",
        "from tensorflow.keras.utils import pad_sequences # se utilizará para padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " en el año 1872, la casa número 7 de saville-row, burlington gardens —donde murió sheridan en 1814— estaba habitada por phileas fogg, quien a pesar de que parecía haber tomado el partido de no hacer nada que pudiese llamar la atención, era uno de los miembros más notables y singulares del reformclub de londres. por consiguiente, phileas fogg, personaje enigmático y del cual sólo se sabía que era un hombre muy galante y de los más cumplidos gentlemen de la alta sociedad inglesa, sucedía a uno de los más grandes oradores que honran a inglaterra. decíase que se daba un aire a lo byron —su cabeza, se entiende, porque, en cuanto a los pies, no tenía defecto alguno—, pero a un byron de bigote y pastillas, a un byron impasible, que hubiera vivido mil años sin envejecer. phileas fogg, era inglés de pura cepa; pero quizás no había nacido en londres. jamás se le había visto en la bolsa ni en el banco, ni en ninguno de los despachos mercantiles de la city. ni las dársenas ni los docks de londres \n"
          ]
        }
      ],
      "source": [
        "clean_article_text = article_text.replace('\\t', '').replace('\\n', '').replace('\\r', '')\n",
        "print(clean_article_text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "573Cg5n7VhWw"
      },
      "outputs": [],
      "source": [
        "# en este caso el vocabulario es el conjunto único de caracteres que existe en todo el texto\n",
        "chars_vocab = set(article_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'\\t',\n",
              " '\\n',\n",
              " '\\r',\n",
              " ' ',\n",
              " '!',\n",
              " '\"',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '/',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " ';',\n",
              " '<',\n",
              " '>',\n",
              " '?',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " '~',\n",
              " '¡',\n",
              " '«',\n",
              " '»',\n",
              " '¿',\n",
              " 'á',\n",
              " 'è',\n",
              " 'é',\n",
              " 'í',\n",
              " 'ñ',\n",
              " 'ó',\n",
              " 'ú',\n",
              " 'ü',\n",
              " '—'}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chars_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VwTK6xgLJd8q"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# la longitud de vocabulario de caracteres es:\n",
        "len(chars_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2W0AeQjXV1Ou"
      },
      "outputs": [],
      "source": [
        "# Construimos los dicionarios que asignan índices a caracteres y viceversa.\n",
        "# El diccionario `char2idx` servirá como tokenizador.\n",
        "char2idx = {k: v for v,k in enumerate(chars_vocab)}\n",
        "idx2char = {v: k for k,v in char2idx.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oIUjVU0LB0r"
      },
      "source": [
        "###  Tokenizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "h07G3srdJppo"
      },
      "outputs": [],
      "source": [
        "# tokenizamos el texto completo\n",
        "tokenized_text = [char2idx[ch] for ch in article_text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PwGVSKOiJ5bj"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[2,\n",
              " 13,\n",
              " 37,\n",
              " 2,\n",
              " 13,\n",
              " 8,\n",
              " 2,\n",
              " 7,\n",
              " 46,\n",
              " 10,\n",
              " 2,\n",
              " 33,\n",
              " 57,\n",
              " 49,\n",
              " 16,\n",
              " 5,\n",
              " 2,\n",
              " 8,\n",
              " 7,\n",
              " 2,\n",
              " 23,\n",
              " 7,\n",
              " 55,\n",
              " 7,\n",
              " 2,\n",
              " 37,\n",
              " 0,\n",
              " 26,\n",
              " 13,\n",
              " 12,\n",
              " 10,\n",
              " 2,\n",
              " 49,\n",
              " 2,\n",
              " 40,\n",
              " 13,\n",
              " 2,\n",
              " 55,\n",
              " 7,\n",
              " 18,\n",
              " 62,\n",
              " 8,\n",
              " 8,\n",
              " 13,\n",
              " 59,\n",
              " 12,\n",
              " 10,\n",
              " 47,\n",
              " 5,\n",
              " 2,\n",
              " 51,\n",
              " 50,\n",
              " 12,\n",
              " 8,\n",
              " 62,\n",
              " 37,\n",
              " 29,\n",
              " 19,\n",
              " 10,\n",
              " 37,\n",
              " 2,\n",
              " 29,\n",
              " 7,\n",
              " 12,\n",
              " 40,\n",
              " 13,\n",
              " 37,\n",
              " 55,\n",
              " 2,\n",
              " 35,\n",
              " 4,\n",
              " 11,\n",
              " 40,\n",
              " 10,\n",
              " 37,\n",
              " 40,\n",
              " 13,\n",
              " 2,\n",
              " 26,\n",
              " 50,\n",
              " 12,\n",
              " 62,\n",
              " 30,\n",
              " 2,\n",
              " 55,\n",
              " 39,\n",
              " 13,\n",
              " 12,\n",
              " 62,\n",
              " 40,\n",
              " 7,\n",
              " 37,\n",
              " 2,\n",
              " 13,\n",
              " 37,\n",
              " 2,\n",
              " 33,\n",
              " 57,\n",
              " 33,\n",
              " 1,\n",
              " 11,\n",
              " 2,\n",
              " 13,\n",
              " 55,\n",
              " 19,\n",
              " 7,\n",
              " 51,\n",
              " 7,\n",
              " 2,\n",
              " 39,\n",
              " 7,\n",
              " 51,\n",
              " 62,\n",
              " 19,\n",
              " 7,\n",
              " 40,\n",
              " 7,\n",
              " 2,\n",
              " 31,\n",
              " 10,\n",
              " 12,\n",
              " 2,\n",
              " 31,\n",
              " 39,\n",
              " 62,\n",
              " 8,\n",
              " 13,\n",
              " 7,\n",
              " 55,\n",
              " 2,\n",
              " 48,\n",
              " 10,\n",
              " 29,\n",
              " 29,\n",
              " 5,\n",
              " 2,\n",
              " 56,\n",
              " 50,\n",
              " 62,\n",
              " 13,\n",
              " 37,\n",
              " 2,\n",
              " 7,\n",
              " 35,\n",
              " 4,\n",
              " 2,\n",
              " 31,\n",
              " 13,\n",
              " 55,\n",
              " 7,\n",
              " 12,\n",
              " 2,\n",
              " 40,\n",
              " 13,\n",
              " 2,\n",
              " 56,\n",
              " 50,\n",
              " 13,\n",
              " 2,\n",
              " 31,\n",
              " 7,\n",
              " 12,\n",
              " 13,\n",
              " 23,\n",
              " 3,\n",
              " 7,\n",
              " 2,\n",
              " 39,\n",
              " 7,\n",
              " 51,\n",
              " 13,\n",
              " 12,\n",
              " 2,\n",
              " 19,\n",
              " 10,\n",
              " 26,\n",
              " 7,\n",
              " 40,\n",
              " 10,\n",
              " 2,\n",
              " 13,\n",
              " 8,\n",
              " 2,\n",
              " 31,\n",
              " 7,\n",
              " 12,\n",
              " 19,\n",
              " 62,\n",
              " 40,\n",
              " 10,\n",
              " 2,\n",
              " 40,\n",
              " 13,\n",
              " 2,\n",
              " 37,\n",
              " 10,\n",
              " 2,\n",
              " 39,\n",
              " 7,\n",
              " 23,\n",
              " 13,\n",
              " 12,\n",
              " 2,\n",
              " 37,\n",
              " 7,\n",
              " 40,\n",
              " 7,\n",
              " 2,\n",
              " 56,\n",
              " 50,\n",
              " 13,\n",
              " 2,\n",
              " 35,\n",
              " 4,\n",
              " 31,\n",
              " 50,\n",
              " 40,\n",
              " 62,\n",
              " 13,\n",
              " 55,\n",
              " 13,\n",
              " 2,\n",
              " 8,\n",
              " 8,\n",
              " 7,\n",
              " 26,\n",
              " 7,\n",
              " 12,\n",
              " 2,\n",
              " 8,\n",
              " 7,\n",
              " 2,\n",
              " 7,\n",
              " 19,\n",
              " 13,\n",
              " 37,\n",
              " 23,\n",
              " 62,\n",
              " 30,\n",
              " 37,\n",
              " 5,\n",
              " 2,\n",
              " 13,\n",
              " 12,\n",
              " 7,\n",
              " 2,\n",
              " 50,\n",
              " 37,\n",
              " 10,\n",
              " 2,\n",
              " 40,\n",
              " 13,\n",
              " 2,\n",
              " 8,\n",
              " 10,\n",
              " 55,\n",
              " 2,\n",
              " 26,\n",
              " 62,\n",
              " 13,\n",
              " 26,\n",
              " 51,\n",
              " 12,\n",
              " 10,\n",
              " 55,\n",
              " 2,\n",
              " 26,\n",
              " 64,\n",
              " 55,\n",
              " 2,\n",
              " 37,\n",
              " 10,\n",
              " 19,\n",
              " 7,\n",
              " 51,\n",
              " 8,\n",
              " 13,\n",
              " 55,\n",
              " 2,\n",
              " 60,\n",
              " 2,\n",
              " 35,\n",
              " 4,\n",
              " 55,\n",
              " 62,\n",
              " 37,\n",
              " 29,\n",
              " 50,\n",
              " 8,\n",
              " 7,\n",
              " 12,\n",
              " 13,\n",
              " 55,\n",
              " 2,\n",
              " 40,\n",
              " 13,\n",
              " 8,\n",
              " 2,\n",
              " 12,\n",
              " 13,\n",
              " 48,\n",
              " 10,\n",
              " 12,\n",
              " 26,\n",
              " 23,\n",
              " 8,\n",
              " 50,\n",
              " 51,\n",
              " 2,\n",
              " 40,\n",
              " 13,\n",
              " 2,\n",
              " 8,\n",
              " 10,\n",
              " 37,\n",
              " 40,\n",
              " 12,\n",
              " 13,\n",
              " 55,\n",
              " 45,\n",
              " 2,\n",
              " 31,\n",
              " 10,\n",
              " 12,\n",
              " 2,\n",
              " 23,\n",
              " 10,\n",
              " 37,\n",
              " 55,\n",
              " 62,\n",
              " 29,\n",
              " 50,\n",
              " 62,\n",
              " 13,\n",
              " 37,\n",
              " 19,\n",
              " 13,\n",
              " 5,\n",
              " 2,\n",
              " 31,\n",
              " 39,\n",
              " 62,\n",
              " 8,\n",
              " 13,\n",
              " 7,\n",
              " 55,\n",
              " 2,\n",
              " 48,\n",
              " 10,\n",
              " 29,\n",
              " 29,\n",
              " 5,\n",
              " 2,\n",
              " 31,\n",
              " 13,\n",
              " 12,\n",
              " 55,\n",
              " 10,\n",
              " 37,\n",
              " 7,\n",
              " 42,\n",
              " 13,\n",
              " 2,\n",
              " 13,\n",
              " 37,\n",
              " 62,\n",
              " 29,\n",
              " 26,\n",
              " 64,\n",
              " 19,\n",
              " 62,\n",
              " 23,\n",
              " 10,\n",
              " 2,\n",
              " 60,\n",
              " 2,\n",
              " 40,\n",
              " 13,\n",
              " 8,\n",
              " 2,\n",
              " 23,\n",
              " 50,\n",
              " 7,\n",
              " 8,\n",
              " 2,\n",
              " 55,\n",
              " 30,\n",
              " 8,\n",
              " 10,\n",
              " 2,\n",
              " 35,\n",
              " 4,\n",
              " 55,\n",
              " 13,\n",
              " 2,\n",
              " 55,\n",
              " 7,\n",
              " 51,\n",
              " 3,\n",
              " 7,\n",
              " 2,\n",
              " 56,\n",
              " 50,\n",
              " 13,\n",
              " 2,\n",
              " 13,\n",
              " 12,\n",
              " 7,\n",
              " 2,\n",
              " 50,\n",
              " 37,\n",
              " 2,\n",
              " 39,\n",
              " 10,\n",
              " 26,\n",
              " 51,\n",
              " 12,\n",
              " 13,\n",
              " 2,\n",
              " 26,\n",
              " 50,\n",
              " 60,\n",
              " 2,\n",
              " 29,\n",
              " 7,\n",
              " 8,\n",
              " 7,\n",
              " 37,\n",
              " 19,\n",
              " 13,\n",
              " 2,\n",
              " 60,\n",
              " 2,\n",
              " 40,\n",
              " 13,\n",
              " 2,\n",
              " 8,\n",
              " 10,\n",
              " 55,\n",
              " 2,\n",
              " 26,\n",
              " 64,\n",
              " 55,\n",
              " 2,\n",
              " 23,\n",
              " 50,\n",
              " 26,\n",
              " 31,\n",
              " 8,\n",
              " 62,\n",
              " 40,\n",
              " 10,\n",
              " 55,\n",
              " 2,\n",
              " 29,\n",
              " 13,\n",
              " 37,\n",
              " 19,\n",
              " 8,\n",
              " 13,\n",
              " 26,\n",
              " 13,\n",
              " 37,\n",
              " 2,\n",
              " 35,\n",
              " 4,\n",
              " 40,\n",
              " 13,\n",
              " 2,\n",
              " 8,\n",
              " 7,\n",
              " 2,\n",
              " 7,\n",
              " 8,\n",
              " 19,\n",
              " 7,\n",
              " 2,\n",
              " 55,\n",
              " 10,\n",
              " 23,\n",
              " 62,\n",
              " 13,\n",
              " 40,\n",
              " 7,\n",
              " 40,\n",
              " 2,\n",
              " 62,\n",
              " 37,\n",
              " 29,\n",
              " 8,\n",
              " 13,\n",
              " 55,\n",
              " 7,\n",
              " 5,\n",
              " 2,\n",
              " 55,\n",
              " 50,\n",
              " 23,\n",
              " 13,\n",
              " 40,\n",
              " 3,\n",
              " 7,\n",
              " 2,\n",
              " 7,\n",
              " 2,\n",
              " 50,\n",
              " 37,\n",
              " 10,\n",
              " 2,\n",
              " 40,\n",
              " 13,\n",
              " 2,\n",
              " 8,\n",
              " 10,\n",
              " 55,\n",
              " 2,\n",
              " 26,\n",
              " 64,\n",
              " 55,\n",
              " 2,\n",
              " 29,\n",
              " 12,\n",
              " 7,\n",
              " 37,\n",
              " 40,\n",
              " 13,\n",
              " 55,\n",
              " 2,\n",
              " 10,\n",
              " 12,\n",
              " 7,\n",
              " 40,\n",
              " 10,\n",
              " 12,\n",
              " 13,\n",
              " 55,\n",
              " 2,\n",
              " 35,\n",
              " 4,\n",
              " 56,\n",
              " 50,\n",
              " 13,\n",
              " 2,\n",
              " 39,\n",
              " 10,\n",
              " 37,\n",
              " 12,\n",
              " 7,\n",
              " 37,\n",
              " 2,\n",
              " 7,\n",
              " 2,\n",
              " 62,\n",
              " 37,\n",
              " 29,\n",
              " 8,\n",
              " 7,\n",
              " 19,\n",
              " 13,\n",
              " 12,\n",
              " 12,\n",
              " 7,\n",
              " 45,\n",
              " 2,\n",
              " 40,\n",
              " 13,\n",
              " 23,\n",
              " 3,\n",
              " 7,\n",
              " 55,\n",
              " 13,\n",
              " 2,\n",
              " 56,\n",
              " 50,\n",
              " 13,\n",
              " 2,\n",
              " 55,\n",
              " 13,\n",
              " 2,\n",
              " 40,\n",
              " 7,\n",
              " 51,\n",
              " 7,\n",
              " 2,\n",
              " 50,\n",
              " 37,\n",
              " 2,\n",
              " 7,\n",
              " 62,\n",
              " 12,\n",
              " 13,\n",
              " 2,\n",
              " 7,\n",
              " 2,\n",
              " 8,\n",
              " 10,\n",
              " 2,\n",
              " 51,\n",
              " 60,\n",
              " 12,\n",
              " 10,\n",
              " 37,\n",
              " 2,\n",
              " 11,\n",
              " 55,\n",
              " 50,\n",
              " 2,\n",
              " 23,\n",
              " 7,\n",
              " 51,\n",
              " 13,\n",
              " 27,\n",
              " 7,\n",
              " 5,\n",
              " 2,\n",
              " 55,\n",
              " 13,\n",
              " 2,\n",
              " 13,\n",
              " 37,\n",
              " 19,\n",
              " 62,\n",
              " 13,\n",
              " 37,\n",
              " 40,\n",
              " 13,\n",
              " 5,\n",
              " 2,\n",
              " 35,\n",
              " 4,\n",
              " 31,\n",
              " 10,\n",
              " 12,\n",
              " 56,\n",
              " 50,\n",
              " 13,\n",
              " 5,\n",
              " 2,\n",
              " 13,\n",
              " 37,\n",
              " 2,\n",
              " 23,\n",
              " 50,\n",
              " 7,\n",
              " 37,\n",
              " 19,\n",
              " 10,\n",
              " 2,\n",
              " 7,\n",
              " 2,\n",
              " 8,\n",
              " 10,\n",
              " 55,\n",
              " 2,\n",
              " 31,\n",
              " 62,\n",
              " 13,\n",
              " 55,\n",
              " 5,\n",
              " 2,\n",
              " 37,\n",
              " 10,\n",
              " 2,\n",
              " 19,\n",
              " 13,\n",
              " 37,\n",
              " 3,\n",
              " 7,\n",
              " 2,\n",
              " 40,\n",
              " 13,\n",
              " 48,\n",
              " 13,\n",
              " 23,\n",
              " 19,\n",
              " 10,\n",
              " 2,\n",
              " 7,\n",
              " 8,\n",
              " 29,\n",
              " 50,\n",
              " 37,\n",
              " 10,\n",
              " 11,\n",
              " 5,\n",
              " 2,\n",
              " 31,\n",
              " 13,\n",
              " 12,\n",
              " 10,\n",
              " 2,\n",
              " 7,\n",
              " 2,\n",
              " 50,\n",
              " 37,\n",
              " 2,\n",
              " 51,\n",
              " 60,\n",
              " 12,\n",
              " 10,\n",
              " 37,\n",
              " 2,\n",
              " 35,\n",
              " 4,\n",
              " 40,\n",
              " 13,\n",
              " 2,\n",
              " 51,\n",
              " 62,\n",
              " 29,\n",
              " 10,\n",
              " 19,\n",
              " 13,\n",
              " 2,\n",
              " 60,\n",
              " 2,\n",
              " 31,\n",
              " 7,\n",
              " 55,\n",
              " 19,\n",
              " 62,\n",
              " 8,\n",
              " 8,\n",
              " 7,\n",
              " 55,\n",
              " 5,\n",
              " 2,\n",
              " 7,\n",
              " 2,\n",
              " 50,\n",
              " 37,\n",
              " 2,\n",
              " 51,\n",
              " 60,\n",
              " 12,\n",
              " 10,\n",
              " 37,\n",
              " 2,\n",
              " 62,\n",
              " 26,\n",
              " 31,\n",
              " 7,\n",
              " 55,\n",
              " 62,\n",
              " 51,\n",
              " 8,\n",
              " 13,\n",
              " 5,\n",
              " 2,\n",
              " 56,\n",
              " 50,\n",
              " 13,\n",
              " 2,\n",
              " 39,\n",
              " 50,\n",
              " 51,\n",
              " 62,\n",
              " 13,\n",
              " 12,\n",
              " 7,\n",
              " 2,\n",
              " 18,\n",
              " 62,\n",
              " 18,\n",
              " 62,\n",
              " 40,\n",
              " 10,\n",
              " 2,\n",
              " 26,\n",
              " 62,\n",
              " 8,\n",
              " 2,\n",
              " 7,\n",
              " 46,\n",
              " 10,\n",
              " 55,\n",
              " 35,\n",
              " 4,\n",
              " 2,\n",
              " 55,\n",
              " 62,\n",
              " 37,\n",
              " 2,\n",
              " 13,\n",
              " 37,\n",
              " 18,\n",
              " 13,\n",
              " 42,\n",
              " 13,\n",
              " 23,\n",
              " 13,\n",
              " 12,\n",
              " 45,\n",
              " 2,\n",
              " 31,\n",
              " 39,\n",
              " 62,\n",
              " 8,\n",
              " 13,\n",
              " 7,\n",
              " 55,\n",
              " 2,\n",
              " 48,\n",
              " 10,\n",
              " 29,\n",
              " 29,\n",
              " 5,\n",
              " 2,\n",
              " 13,\n",
              " 12,\n",
              " 7,\n",
              " 2,\n",
              " 62,\n",
              " 37,\n",
              " 29,\n",
              " 8,\n",
              " 44,\n",
              " 55,\n",
              " 2,\n",
              " 40,\n",
              " 13,\n",
              " 2,\n",
              " 31,\n",
              " 50,\n",
              " 12,\n",
              " 7,\n",
              " 2,\n",
              " 23,\n",
              " 13,\n",
              " 31,\n",
              " 7,\n",
              " 20,\n",
              " 2,\n",
              " 31,\n",
              " 13,\n",
              " 12,\n",
              " 10,\n",
              " 2,\n",
              " 56,\n",
              " 50,\n",
              " 62,\n",
              " 27,\n",
              " 64,\n",
              " 55,\n",
              " 2,\n",
              " 37,\n",
              " 10,\n",
              " 2,\n",
              " 39,\n",
              " 7,\n",
              " 51,\n",
              " 3,\n",
              " 7,\n",
              " 2,\n",
              " 37,\n",
              " 7,\n",
              " 23,\n",
              " 62,\n",
              " 40,\n",
              " 10,\n",
              " 2,\n",
              " 13,\n",
              " 37,\n",
              " 35,\n",
              " 4,\n",
              " 2,\n",
              " 8,\n",
              " 10,\n",
              " 37,\n",
              " 40,\n",
              " 12,\n",
              " 13,\n",
              " 55,\n",
              " 45,\n",
              " 2,\n",
              " 42,\n",
              " 7,\n",
              " 26,\n",
              " 64,\n",
              " 55,\n",
              " 2,\n",
              " 55,\n",
              " 13,\n",
              " 2,\n",
              " 8,\n",
              " 13,\n",
              " 2,\n",
              " 39,\n",
              " 7,\n",
              " 51,\n",
              " 3,\n",
              " 7,\n",
              " 2,\n",
              " 18,\n",
              " 62,\n",
              " 55,\n",
              " 19,\n",
              " 10,\n",
              " 2,\n",
              " 13,\n",
              " 37,\n",
              " 2,\n",
              " 8,\n",
              " 7,\n",
              " 2,\n",
              " 51,\n",
              " 10,\n",
              " 8,\n",
              " 55,\n",
              " 7,\n",
              " 2,\n",
              " 37,\n",
              " 62,\n",
              " 2,\n",
              " 13,\n",
              " 37,\n",
              " 2,\n",
              " 13,\n",
              " 8,\n",
              " 2,\n",
              " 51,\n",
              " 7,\n",
              " 37,\n",
              " 23,\n",
              " 10,\n",
              " 5,\n",
              " 2,\n",
              " 37,\n",
              " 62,\n",
              " 2,\n",
              " 13,\n",
              " 37,\n",
              " 2,\n",
              " 35,\n",
              " 4,\n",
              " 37,\n",
              " 62,\n",
              " 37,\n",
              " 29,\n",
              " 50,\n",
              " 37,\n",
              " 10,\n",
              " 2,\n",
              " 40,\n",
              " 13,\n",
              " 2,\n",
              " 8,\n",
              " 10,\n",
              " 55,\n",
              " 2,\n",
              " 40,\n",
              " 13,\n",
              " 55,\n",
              " 31,\n",
              " 7,\n",
              " 23,\n",
              " 39,\n",
              " 10,\n",
              " 55,\n",
              " 2,\n",
              " 26,\n",
              " 13,\n",
              " 12,\n",
              " 23,\n",
              " 7,\n",
              " 37,\n",
              " 19,\n",
              " 62,\n",
              " 8,\n",
              " 13,\n",
              " 55,\n",
              " 2,\n",
              " 40,\n",
              " 13,\n",
              " 2,\n",
              " 8,\n",
              " 7,\n",
              " 2,\n",
              " 23,\n",
              " 62,\n",
              " 19,\n",
              " 60,\n",
              " 45,\n",
              " 2,\n",
              " 37,\n",
              " 62,\n",
              " 2,\n",
              " 8,\n",
              " 7,\n",
              " 55,\n",
              " 2,\n",
              " 40,\n",
              " 64,\n",
              " 12,\n",
              " 55,\n",
              " 13,\n",
              " 37,\n",
              " 7,\n",
              " 55,\n",
              " 2]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_text[:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfpYcaypKcI9"
      },
      "source": [
        "### Organizando y estructurando el dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WSSmg9jtKP0T"
      },
      "outputs": [],
      "source": [
        "# separaremos el dataset entre entrenamiento y validación.\n",
        "# `p_val` será la proporción del corpus que se reservará para validación\n",
        "# `num_val` es la cantidad de secuencias de tamaño `max_context_size` que se usará en validación\n",
        "p_val = 0.1\n",
        "num_val = int(np.ceil(len(tokenized_text)*p_val/max_context_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "b7dCpGrdKll0"
      },
      "outputs": [],
      "source": [
        "# separamos la porción de texto utilizada en entrenamiento de la de validación.\n",
        "train_text = tokenized_text[:-num_val*max_context_size]\n",
        "val_text = tokenized_text[-num_val*max_context_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NmxQdxl8LRCg"
      },
      "outputs": [],
      "source": [
        "tokenized_sentences_val = [val_text[init*max_context_size:init*(max_context_size+1)] for init in range(num_val)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_gyFT9koLqDm"
      },
      "outputs": [],
      "source": [
        "tokenized_sentences_train = [train_text[init:init+max_context_size] for init in range(len(train_text)-max_context_size+1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "oVNqmmLRodT0"
      },
      "outputs": [],
      "source": [
        "X = np.array(tokenized_sentences_train[:-1])\n",
        "y = np.array(tokenized_sentences_train[1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vken7O4ETsAJ"
      },
      "source": [
        "Nótese que estamos estructurando el problema de aprendizaje como *many-to-many*:\n",
        "\n",
        "Entrada: secuencia de tokens [$x_0$, $x_1$, ..., $x_N$]\n",
        "\n",
        "Target: secuencia de tokens [$x_1$, $x_2$, ..., $x_{N+1}$]\n",
        "\n",
        "De manera que la red tiene que aprender que su salida deben ser los tokens desplazados en una posición y un nuevo token predicho (el N+1).\n",
        "\n",
        "La ventaja de estructurar el aprendizaje de esta manera es que para cada token de target se propaga una señal de gradiente por el grafo de cómputo recurrente, que es mejor que estructurar el problema como *many-to-one* en donde sólo una señal de gradiente se propaga."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3iPTx-UJl6r"
      },
      "source": [
        "En este punto tenemos en la variable `tokenized_sentences` los versos tokenizados. Vamos a quedarnos con un conjunto de validación que utilizaremos para medir la calidad de la generación de secuencias con la métrica de Perplejidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "KFAyA4zCWE-5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(359671, 100)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qcKRl70HFTzG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 2, 13, 37,  2, 13,  8,  2,  7, 46, 10])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X[0,:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "TVpLCKSZFXZO"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([13, 37,  2, 13,  8,  2,  7, 46, 10,  2])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y[0,:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wOFCR-KqbW1N"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(chars_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnnjdAQ5UAEJ"
      },
      "source": [
        "# Definiendo el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "rkMCZvmhrQz4"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input, TimeDistributed, CategoryEncoding, SimpleRNN, Dense\n",
        "from keras.models import Model, Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgz7VKwTUbj6"
      },
      "source": [
        "El modelo que se propone como ejemplo consume los índices de los tokens y los transforma en vectores OHE (en este caso no entrenamos una capa de embedding para caracteres). Esa transformación se logra combinando las capas `CategoryEncoding` que transforma a índices a vectores OHE y `TimeDistributed` que aplica la capa a lo largo de la dimensión \"temporal\" de la secuencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Zd2OkfQYs2Q7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nacho/.local/lib/python3.10/site-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "E0000 00:00:1744918397.244737   46974 cuda_executor.cc:1228] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
            "W0000 00:00:1744918397.246058   46974 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">53,800</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">13,668</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m68\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)      │        \u001b[38;5;34m53,800\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m68\u001b[0m)       │        \u001b[38;5;34m13,668\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,468</span> (263.55 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m67,468\u001b[0m (263.55 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,468</span> (263.55 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m67,468\u001b[0m (263.55 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(TimeDistributed(CategoryEncoding(num_tokens=vocab_size, output_mode = \"one_hot\"),input_shape=(None,1)))\n",
        "model.add(SimpleRNN(200, return_sequences=True, dropout=0.1, recurrent_dropout=0.1 ))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmJWNyxQwfCE"
      },
      "source": [
        "\n",
        "### Definir el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWK3z85sQfUe"
      },
      "source": [
        "Dado que por el momento no hay implementaciones adecuadas de la perplejidad que puedan operar en tiempo de entrenamiento, armaremos un Callback *ad-hoc* que la calcule en cada epoch.\n",
        "\n",
        "**Nota**: un Callback es una rutina gatillada por algún evento, son muy útiles para relevar datos en diferentes momentos del desarrollo del modelo. En este caso queremos hacer un cálculo cada vez que termina una epoch de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zUHX3r5JD-MG"
      },
      "outputs": [],
      "source": [
        "class PplCallback(keras.callbacks.Callback):\n",
        "\n",
        "    '''\n",
        "    Este callback es una solución ad-hoc para calcular al final de cada epoch de\n",
        "    entrenamiento la métrica de Perplejidad sobre un conjunto de datos de validación.\n",
        "    La perplejidad es una métrica cuantitativa para evaluar la calidad de la generación de secuencias.\n",
        "    Además implementa la finalización del entrenamiento (Early Stopping)\n",
        "    si la perplejidad no mejora después de `patience` epochs.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, val_data, history_ppl,patience=5):\n",
        "      # El callback lo inicializamos con secuencias de validación sobre las cuales\n",
        "      # mediremos la perplejidad\n",
        "      self.val_data = val_data\n",
        "\n",
        "      self.target = []\n",
        "      self.padded = []\n",
        "\n",
        "      count = 0\n",
        "      self.info = []\n",
        "      self.min_score = np.inf\n",
        "      self.patience_counter = 0\n",
        "      self.patience = patience\n",
        "\n",
        "      # nos movemos en todas las secuencias de los datos de validación\n",
        "      for seq in self.val_data:\n",
        "\n",
        "        len_seq = len(seq)\n",
        "        # armamos todas las subsecuencias\n",
        "        subseq = [seq[:i] for i in range(1,len_seq)]\n",
        "        self.target.extend([seq[i] for i in range(1,len_seq)])\n",
        "\n",
        "        if len(subseq)!=0:\n",
        "\n",
        "          self.padded.append(pad_sequences(subseq, maxlen=max_context_size, padding='pre'))\n",
        "\n",
        "          self.info.append((count,count+len_seq))\n",
        "          count += len_seq\n",
        "\n",
        "      self.padded = np.vstack(self.padded)\n",
        "\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "        # en `scores` iremos guardando la perplejidad de cada secuencia\n",
        "        scores = []\n",
        "\n",
        "        predictions = self.model.predict(self.padded,verbose=0)\n",
        "\n",
        "        # para cada secuencia de validación\n",
        "        for start,end in self.info:\n",
        "\n",
        "          # en `probs` iremos guardando las probabilidades de los términos target\n",
        "          probs = [predictions[idx_seq,-1,idx_vocab] for idx_seq, idx_vocab in zip(range(start,end),self.target[start:end])]\n",
        "\n",
        "          # calculamos la perplejidad por medio de logaritmos\n",
        "          scores.append(np.exp(-np.sum(np.log(probs))/(end-start)))\n",
        "\n",
        "        # promediamos todos los scores e imprimimos el valor promedio\n",
        "        current_score = np.mean(scores)\n",
        "        history_ppl.append(current_score)\n",
        "        print(f'\\n mean perplexity: {current_score} \\n')\n",
        "\n",
        "        # chequeamos si tenemos que detener el entrenamiento\n",
        "        if current_score < self.min_score:\n",
        "          self.min_score = current_score\n",
        "          self.model.save(\"my_model.keras\")\n",
        "          print(\"Saved new model!\")\n",
        "          self.patience_counter = 0\n",
        "        else:\n",
        "          self.patience_counter += 1\n",
        "          if self.patience_counter == self.patience:\n",
        "            print(\"Stopping training...\")\n",
        "            self.model.stop_training = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "\n",
        "class PplCallback(keras.callbacks.Callback):\n",
        "\n",
        "    '''\n",
        "    Este callback es una solución ad-hoc para calcular al final de cada epoch de\n",
        "    entrenamiento la métrica de Perplejidad sobre un conjunto de datos de validación.\n",
        "    La perplejidad es una métrica cuantitativa para evaluar la calidad de la generación de secuencias.\n",
        "    Además implementa la finalización del entrenamiento (Early Stopping)\n",
        "    si la perplejidad no mejora después de `patience` epochs.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, val_data, history_ppl,patience=5, name_model=\"my_model.keras\"):\n",
        "      # El callback lo inicializamos con secuencias de validación sobre las cuales\n",
        "      # mediremos la perplejidad\n",
        "      self.val_data = val_data\n",
        "\n",
        "      self.target = []\n",
        "      self.padded = []\n",
        "\n",
        "      count = 0\n",
        "      self.info = []\n",
        "      self.min_score = np.inf\n",
        "      self.patience_counter = 0\n",
        "      self.history_ppl = history_ppl\n",
        "      self.patience = patience\n",
        "      self.name_model = name_model\n",
        "\n",
        "      # nos movemos en todas las secuencias de los datos de validación\n",
        "      for seq in self.val_data:\n",
        "\n",
        "        len_seq = len(seq)\n",
        "        # armamos todas las subsecuencias\n",
        "        subseq = [seq[:i] for i in range(1,len_seq)]\n",
        "        self.target.extend([seq[i] for i in range(1,len_seq)])\n",
        "\n",
        "        if len(subseq)!=0:\n",
        "\n",
        "          self.padded.append(pad_sequences(subseq, maxlen=max_context_size, padding='pre'))\n",
        "\n",
        "          self.info.append((count,count+len_seq))\n",
        "          count += len_seq\n",
        "\n",
        "      self.padded = np.vstack(self.padded)\n",
        "\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "        # en `scores` iremos guardando la perplejidad de cada secuencia\n",
        "        scores = []\n",
        "\n",
        "        predictions = self.model.predict(self.padded,verbose=0)\n",
        "\n",
        "        # para cada secuencia de validación\n",
        "        print(self.name_model)\n",
        "        for start,end in self.info:\n",
        "\n",
        "          # en `probs` iremos guardando las probabilidades de los términos target\n",
        "          probs = [predictions[idx_seq,-1,idx_vocab] for idx_seq, idx_vocab in zip(range(start,end),self.target[start:end])]\n",
        "\n",
        "          # calculamos la perplejidad por medio de logaritmos\n",
        "          scores.append(np.exp(-np.sum(np.log(probs))/(end-start)))\n",
        "\n",
        "        # promediamos todos los scores e imprimimos el valor promedio\n",
        "        current_score = np.mean(scores)\n",
        "        self.history_ppl.append(current_score)\n",
        "        print(f'\\n mean perplexity: {current_score} \\n')\n",
        "\n",
        "        # chequeamos si tenemos que detener el entrenamiento\n",
        "        if current_score < self.min_score:\n",
        "          self.min_score = current_score\n",
        "          self.model.save(self.name_model)\n",
        "          print(\"Saved new model!\")\n",
        "          self.patience_counter = 0\n",
        "        else:\n",
        "          self.patience_counter += 1\n",
        "          if self.patience_counter == self.patience:\n",
        "            print(\"Stopping training...\")\n",
        "            self.model.stop_training = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HBZIwR0gruA"
      },
      "source": [
        "### Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQq1PHDkxDvN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - loss: 2.4380"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-17 16:39:06.820092: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2143550400 exceeds 10% of free system memory.\n"
          ]
        }
      ],
      "source": [
        "# fiteamos, nótese el agregado del callback con su inicialización. El batch_size lo podemos seleccionar a mano\n",
        "# en general, lo mejor es escoger el batch más grande posible que minimice el tiempo de cada época.\n",
        "# En la variable `history_ppl` se guardarán los valores de perplejidad para cada época.\n",
        "history_ppl = []\n",
        "hist = model.fit(X, y, epochs=20, callbacks=[PplCallback(tokenized_sentences_val,history_ppl)], batch_size=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K30JHB3Dv-mx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Entrenamiento\n",
        "epoch_count = range(1, len(history_ppl) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=history_ppl)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rhy5hZN38qfO"
      },
      "outputs": [],
      "source": [
        "# Cargamos el mejor modelo guardado del entrenamiento para hacer inferencia\n",
        "model = keras.models.load_model('my_model.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN6Fg_BsxJe6"
      },
      "source": [
        "\n",
        "### Predicción del próximo caracter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBvKHFPmzpy2"
      },
      "outputs": [],
      "source": [
        "# Se puede usar gradio para probar el modelo\n",
        "# Gradio es una herramienta muy útil para crear interfaces para ensayar modelos\n",
        "# https://gradio.app/\n",
        "\n",
        "!pip install -q gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNyBykvhzs7-"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def model_response(human_text):\n",
        "\n",
        "    # Encodeamos\n",
        "    encoded = [char2idx[ch] for ch in human_text.lower() ]\n",
        "    # Si tienen distinto largo\n",
        "    encoded = pad_sequences([encoded], maxlen=max_context_size, padding='pre')\n",
        "\n",
        "    # Predicción softmax\n",
        "    y_hat = np.argmax(model.predict(encoded)[0,-1,:])\n",
        "\n",
        "\n",
        "    # Debemos buscar en el vocabulario el caracter\n",
        "    # que corresopnde al indice (y_hat) predicho por le modelo\n",
        "    out_word = ''\n",
        "    out_word = idx2char[y_hat]\n",
        "\n",
        "    # Agrego la palabra a la frase predicha\n",
        "    return human_text + out_word\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=model_response,\n",
        "    inputs=[\"textbox\"],\n",
        "    outputs=\"text\")\n",
        "\n",
        "iface.launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCeMWWupxN1-"
      },
      "source": [
        "### Generación de secuencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwbS_pfhxvB3"
      },
      "outputs": [],
      "source": [
        "def generate_seq(model, seed_text, max_length, n_words):\n",
        "    \"\"\"\n",
        "        Exec model sequence prediction\n",
        "\n",
        "        Args:\n",
        "            model (keras): modelo entrenado\n",
        "            seed_text (string): texto de entrada (input_seq)\n",
        "            max_length (int): máxima longitud de la sequencia de entrada\n",
        "            n_words (int): números de caracteres a agregar a la sequencia de entrada\n",
        "        returns:\n",
        "            output_text (string): sentencia con las \"n_words\" agregadas\n",
        "    \"\"\"\n",
        "    output_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "    for _ in range(n_words):\n",
        "\t\t# Encodeamos\n",
        "        encoded = [char2idx[ch] for ch in output_text.lower() ]\n",
        "\t\t# Si tienen distinto largo\n",
        "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "\n",
        "\t\t# Predicción softmax\n",
        "        y_hat = np.argmax(model.predict(encoded,verbose=0)[0,-1,:])\n",
        "\t\t# Vamos concatenando las predicciones\n",
        "        out_word = ''\n",
        "\n",
        "        out_word = idx2char[y_hat]\n",
        "\n",
        "\t\t# Agrego las palabras a la frase predicha\n",
        "        output_text += out_word\n",
        "    return output_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoFqRC5pxzqS"
      },
      "outputs": [],
      "source": [
        "input_text='habia una vez'\n",
        "\n",
        "generate_seq(model, input_text, max_length=max_context_size, n_words=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drJ6xn5qW1Hl"
      },
      "source": [
        "###  Beam search y muestreo aleatorio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vovn9XZW1Hl"
      },
      "outputs": [],
      "source": [
        "# funcionalidades para hacer encoding y decoding\n",
        "\n",
        "def encode(text,max_length=max_context_size):\n",
        "\n",
        "    encoded = [char2idx[ch] for ch in text]\n",
        "    encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "\n",
        "    return encoded\n",
        "\n",
        "def decode(seq):\n",
        "    return ''.join([idx2char[ch] for ch in seq])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_lZiQwkW1Hl"
      },
      "outputs": [],
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "# función que selecciona candidatos para el beam search\n",
        "def select_candidates(pred,num_beams,vocab_size,history_probs,history_tokens,temp,mode):\n",
        "\n",
        "  # colectar todas las probabilidades para la siguiente búsqueda\n",
        "  pred_large = []\n",
        "\n",
        "  for idx,pp in enumerate(pred):\n",
        "    pred_large.extend(np.log(pp+1E-10)+history_probs[idx])\n",
        "\n",
        "  pred_large = np.array(pred_large)\n",
        "\n",
        "  # criterio de selección\n",
        "  if mode == 'det':\n",
        "    idx_select = np.argsort(pred_large)[::-1][:num_beams] # beam search determinista\n",
        "  elif mode == 'sto':\n",
        "    idx_select = np.random.choice(np.arange(pred_large.shape[0]), num_beams, p=softmax(pred_large/temp)) # beam search con muestreo aleatorio\n",
        "  else:\n",
        "    raise ValueError(f'Wrong selection mode. {mode} was given. det and sto are supported.')\n",
        "\n",
        "  # traducir a índices de token en el vocabulario\n",
        "  new_history_tokens = np.concatenate((np.array(history_tokens)[idx_select//vocab_size],\n",
        "                        np.array([idx_select%vocab_size]).T),\n",
        "                      axis=1)\n",
        "\n",
        "  # devolver el producto de las probabilidades (log) y la secuencia de tokens seleccionados\n",
        "  return pred_large[idx_select.astype(int)], new_history_tokens.astype(int)\n",
        "\n",
        "\n",
        "def beam_search(model,num_beams,num_words,input,temp=1,mode='det'):\n",
        "\n",
        "    # first iteration\n",
        "\n",
        "    # encode\n",
        "    encoded = encode(input)\n",
        "\n",
        "    # first prediction\n",
        "    y_hat = model.predict(encoded,verbose=0)[0,-1,:]\n",
        "\n",
        "    # get vocabulary size\n",
        "    vocab_size = y_hat.shape[0]\n",
        "\n",
        "    # initialize history\n",
        "    history_probs = [0]*num_beams\n",
        "    history_tokens = [encoded[0]]*num_beams\n",
        "\n",
        "    # select num_beams candidates\n",
        "    history_probs, history_tokens = select_candidates([y_hat],\n",
        "                                        num_beams,\n",
        "                                        vocab_size,\n",
        "                                        history_probs,\n",
        "                                        history_tokens,\n",
        "                                        temp,\n",
        "                                        mode)\n",
        "\n",
        "    # beam search loop\n",
        "    for i in range(num_words-1):\n",
        "\n",
        "      preds = []\n",
        "\n",
        "      for hist in history_tokens:\n",
        "\n",
        "        # actualizar secuencia de tokens\n",
        "        input_update = np.array([hist[i+1:]]).copy()\n",
        "\n",
        "        # predicción\n",
        "        y_hat = model.predict(input_update,verbose=0)[0,-1,:]\n",
        "\n",
        "        preds.append(y_hat)\n",
        "\n",
        "      history_probs, history_tokens = select_candidates(preds,\n",
        "                                                        num_beams,\n",
        "                                                        vocab_size,\n",
        "                                                        history_probs,\n",
        "                                                        history_tokens,\n",
        "                                                        temp,\n",
        "                                                        mode)\n",
        "\n",
        "    return history_tokens[:,-(len(input)+num_words):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeLqAoOYW1Hm"
      },
      "outputs": [],
      "source": [
        "# predicción con beam search\n",
        "salidas = beam_search(model,num_beams=10,num_words=20,input=\"habia una vez\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8HQoLhw-NYg"
      },
      "outputs": [],
      "source": [
        "salidas[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2S3_I3S1W1Hm"
      },
      "outputs": [],
      "source": [
        "# veamos las salidas\n",
        "decode(salidas[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_LlqmtEW1Hn"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
